# Carbon- and Precedence-Aware Scheduling for Data Processing
## Simulator Code

The Python code in this directory forks the Decima simulator found at [hongzimao/decima-sim](https://github.com/hongzimao/decima-sim), which simulates a realistic Spark cluster for the evaluation and training of scheduling policies.

To evaluate `PCAPS` and `CAP` against all implemented baselines in a simulated cluster with 100 executors and one trial of 50 random TPC-H jobs under the sample carbon trace, run the following command at the root of this repository:

```bash
python3 simulator/test.py --num_exp 1 --exec_cap 50 --exec_cap 100 --num_init_dags 1 --num_stream_dags 50 --canvs_visualization 0 --test_schemes spark_fifo dynamic_partition decima green_hadoop cap_fifo cap_partition cap_decima pcaps
```

The simulator code assumes a Python environment with the following packages:
- [NumPy](https://numpy.org)
- [SciPy](https://scipy.org)
- [pandas](https://pandas.pydata.org)
- [networkx](https://networkx.org)
- [TensorFlow](https://www.tensorflow.org) for Decima inference
- [tf-slim](https://github.com/google-research/tf-slim)
- [Matplotlib](https://matplotlib.org) for creating plots 
- [seaborn](https://seaborn.pydata.org) for creating plots 

### Directory Structure Highlights

- **`agents/`**  
  Contains implementations of each scheduling policy, including `PCAPS` and `CAP`.
  - **`agent.py`**  
    Defines the base class for all scheduling agents.
  - **`actor-agent.py`**
    Implements the [Decima](https://web.mit.edu/decima/) scheduler.
  - **`spark_agent.py`**
    Implements the default Spark FIFO scheduler.
  - **`heuristic_agent.py`**
    Implements the heuristic Weighted Fair scheduler.
  - **`carbon_aware_actor_agent.py`**
    Implements `CAP` on top of the Decima scheduler.
  - **`carbon_aware_fifo_agent.py`**
    Implements `CAP` on top of the default Spark FIFO scheduler.
  - **`carbon_aware_heuristic_agent.py`**
    Implements `CAP` on top of the Weighted Fair scheduler.
  - **`pcaps_actor_agent.py`**
    Implements `PCAPS` using Decima as the underlying probabilistic scheduler.
  - **`green_hadoop_agent.py`**
    Implements an adapted [GreenHadoop](https://dl.acm.org/doi/10.1145/2168836.2168843) scheduler for the Spark scheduling setting.

- **`models/`**  
  Pretrained model weights for the Decima scheduler.

- **`results/`**  
  By default, any plots or results generated by the simulator are saved here.

- **`spark_env/`**  
  The primary implementation of the Spark environment, including definitions for TPC-H example jobs, and `canvas.py`, which is used to plot results.

- **`test.py`**  
  The primary script to run a trial in the simulation environment -- this accepts command-line parameters (all explicitly defined in `param.py`) to adjust e.g., the number of executors, number of jobs, or the schedulers to test.  This reports the carbon footprint, average job completion time, and end-to-end completion time for each scheduler at the command line and in a plot.